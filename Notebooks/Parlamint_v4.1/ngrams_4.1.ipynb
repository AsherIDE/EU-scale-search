{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b903477a",
   "metadata": {},
   "source": [
    "# Debate a base\n",
    "#### Update script (Parlamint 4.1)\n",
    "\n",
    "\n",
    "### Functions\n",
    "- Uploads all Parlamint data to ElasticSearch for the `ngrams` page\n",
    "- BEWARE THIS WILL TAKE DAYS\n",
    "\n",
    "### Don't forget to:\n",
    "- Make sure that nothing else inside of the `data/original/EU` and `data/original/EN` folders besides what was mentioned in the step-by-step guide\n",
    "- Fill in `es_host`, `es_user` and `es_password` so you can connect with your ElasticSearch instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c721fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in credentials over here!\n",
    "es_host = \"https://localhost:9200\"\n",
    "es_user = \"CHANGEME\"\n",
    "es_password = \"CHANGEME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246010f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5996d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# establish connection with elasticsearch\n",
    "\n",
    "# Elastic host\n",
    "es = Elasticsearch(\n",
    "    hosts=[\n",
    "            es_host\n",
    "    ],\n",
    "    http_auth=(es_user, es_password),\n",
    "#     use_ssl=True,\n",
    "    verify_certs=False,\n",
    "#     ca_certs=\"./ca.crt\"\n",
    "    timeout=30, \n",
    "    max_retries=10, \n",
    "    retry_on_timeout=True\n",
    ")\n",
    "\n",
    "# dir\n",
    "translated_csv_dir = \"../data/preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79dc0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is VERY dangerous to uncomment this\n",
    "# es.indices.delete(index='ngrams1')\n",
    "# es.indices.delete(index='ngrams2')\n",
    "# es.indices.delete(index='ngrams3')\n",
    "# es.indices.delete(index='ngrams4')\n",
    "# es.indices.delete(index='ngrams5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adeb4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list met dict [{Land1}, {Land 1}]\n",
    "def get_csv_files(country_selection):\n",
    "    # os.chdir(translated_csv_dir)\n",
    "    \n",
    "    country_return_list = []\n",
    "\n",
    "    # loop door alle folders die hierboven zijn geprint\n",
    "    for country in os.listdir(translated_csv_dir):\n",
    "        \n",
    "        # filter op specifiek land (IN BOX 2)\n",
    "        if country == country_selection:\n",
    "            paths_dict = {}\n",
    "\n",
    "            # ga door alle inhoud van de landfolder heen\n",
    "            for root, dirs, files in os.walk(os.path.join(translated_csv_dir, country)):\n",
    "                file_data = []\n",
    "                \n",
    "                # loop door files van een folder\n",
    "                for file in files:\n",
    "                    \n",
    "                    file_data.append(file)\n",
    "\n",
    "                paths_dict[root.split(\"\\\\\")[1]] = file_data\n",
    "\n",
    "            country_return_list.append(paths_dict)\n",
    "        \n",
    "    return country_return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d6eeb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# haal jaar maand en dag uit filename\n",
    "def extract_file_date(file_name):\n",
    "    \n",
    "    year_month_day = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", file_name)\n",
    "#     year, month, day = year_month_day[0].split(\"-\")\n",
    "    \n",
    "    return year_month_day[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c538b90e",
   "metadata": {},
   "source": [
    "### Shingles from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69b14a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# geeft een dict met dates als keys en files als vals\n",
    "def get_files_per_date(country, after=None):\n",
    "    files = get_csv_files(country)[0][country]\n",
    "    \n",
    "    dates_dict = {}\n",
    "    \n",
    "    if after is not None:\n",
    "        \n",
    "        after = time.mktime(datetime.datetime.strptime(after, \"%Y-%m-%d\").timetuple())\n",
    "    \n",
    "    # loop door alle files heen\n",
    "    for file in files:\n",
    "        \n",
    "        date = extract_file_date(file)\n",
    "        \n",
    "        # filter al geuploade dates uit de dict\n",
    "        if after is not None:\n",
    "            timestamp = time.mktime(datetime.datetime.strptime(date, \"%Y-%m-%d\").timetuple())\n",
    "            \n",
    "            if (timestamp - after) <= 0:\n",
    "                \n",
    "                continue\n",
    "        \n",
    "        if date in dates_dict.keys():\n",
    "            \n",
    "            dates_dict[date] += [file]\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            dates_dict[date] = [file]\n",
    "            \n",
    "    return dates_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd195e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maakt paths aan voor alle files die bij een date horen\n",
    "def get_date_paths(root, files):\n",
    "    # os.chdir(translated_csv_dir)\n",
    "    \n",
    "    paths = []\n",
    "    \n",
    "    for file in files:\n",
    "        paths.append(os.path.join(translated_csv_dir, root, file))\n",
    "        \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63e7234b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maakt ngrams van alle tekst in een csv file\n",
    "def get_date_vocabulary(paths):\n",
    "    # os.chdir(translated_csv_dir)\n",
    "    \n",
    "    vocabulary = np.array(())\n",
    "    \n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "    \n",
    "        # slecht vertaalde notebooks met alleen een index negeren\n",
    "        if len(df.index) == 0:\n",
    "            continue\n",
    "            \n",
    "        lines = [x for x in df[\"value\"].to_list() if str(x) != 'nan']\n",
    "    \n",
    "        victor = CountVectorizer(ngram_range=(1, 5), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "        victor.fit_transform(lines)\n",
    "        \n",
    "        vocabulary = np.unique(np.hstack((vocabulary, victor.get_feature_names_out()))) \n",
    "        \n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae84629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_counts(paths, vocabulary):\n",
    "    # os.chdir(translated_csv_dir)\n",
    "    \n",
    "    counts = np.zeros(shape=(len(vocabulary)), dtype=int)\n",
    "    \n",
    "    for path in paths:\n",
    "        df = pd.read_csv(path)\n",
    "    \n",
    "        # slecht vertaalde notebooks met alleen een index negeren\n",
    "        if len(df.index) == 0:\n",
    "            continue\n",
    "            \n",
    "        lines = [x for x in df[\"value\"].to_list() if str(x) != 'nan']\n",
    "    \n",
    "        victor = CountVectorizer(ngram_range=(1, 5), token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "        victor.fit_transform(vocabulary)\n",
    "        \n",
    "        counts = counts + np.sum(victor.transform(lines).toarray(), axis=0)\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "541b0006",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_es_date_dict(vocabulary, country, date, counts, index=\"ngrams\"):\n",
    "    year, month, day = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", date)[0].split(\"-\")\n",
    "    \n",
    "    bulk = []\n",
    "    \n",
    "    for ngram, count in zip(vocabulary, counts):\n",
    "        content_dict, es_content_dict = {}, {}\n",
    "        \n",
    "        # entry inhoud\n",
    "        content_dict[\"ngram\"] = ngram\n",
    "        content_dict[\"country\"] = country\n",
    "        content_dict[\"year\"] = year\n",
    "        content_dict[\"month\"] = month\n",
    "        content_dict[\"day\"] = day\n",
    "        content_dict[\"count\"] = count\n",
    "        \n",
    "        # entry technische dingen\n",
    "        es_content_dict[\"_index\"] = f\"{index}{len(ngram.split(' '))}\"\n",
    "        es_content_dict[\"_source\"] = content_dict\n",
    "        \n",
    "        # voeg entry toe aan bulk return lijst\n",
    "        bulk.append(es_content_dict)\n",
    "        \n",
    "    return bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41b7491b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # functie testruimte\n",
    "# t_folder = \"BE\"\n",
    "# t_date = '2014-11-18'\n",
    "\n",
    "# test_dates_dict = get_files_per_date(t_folder)\n",
    "# print(test_dates_dict)\n",
    "# test_paths = get_date_paths(t_folder, test_dates_dict[t_date ])\n",
    "# print(test_paths)\n",
    "# test_vocabulary = get_date_vocabulary(test_paths)\n",
    "# print(test_vocabulary)\n",
    "# test_counts = get_date_counts(test_paths, test_vocabulary)\n",
    "# print(test_counts)\n",
    "# test_date_es_dict = get_es_date_dict(test_vocabulary, t_folder, t_date, test_counts, 0)\n",
    "# print(test_date_es_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9396025",
   "metadata": {},
   "source": [
    "### Error management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5148b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loc = os.path.join(\"../data\", \"uploaded_ngram_dates.csv\")\n",
    "\n",
    "# houd bij welke files zijn geupload tijdens multiprocessing\n",
    "def log_date_upload(country, date):\n",
    "    with open(log_loc, 'a', newline='', encoding=\"utf-8\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([country, date])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f9ab25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter de dates die al indexed zijn\n",
    "def get_remaining_dates(country, dates):\n",
    "    df_processed_dates = pd.read_csv(os.path.join(\"../data\", \"uploaded_ngram_dates.csv\"))\n",
    "    \n",
    "    df_processed_dates_cutout = df_processed_dates[df_processed_dates[\"key\"] == country]\n",
    "    \n",
    "    if len(df_processed_dates_cutout) != 0:\n",
    "        disposable_dates = list(df_processed_dates_cutout[\"value\"])\n",
    "        \n",
    "        for disposable_date in disposable_dates:\n",
    "            if disposable_date in dates.keys():\n",
    "                dates.pop(disposable_date)\n",
    "                \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1772ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Countvectorizer 1.3\n",
    "# Elastic (index = ngrams):\n",
    "# - | Shingle | Land | Jaar | Maand | Dag | Aantal | Percentage |\n",
    "#  - Shingles doen voor een datum, voor een land\n",
    "#  - Percentage berekenen\n",
    "#  - Uploaden in ES\n",
    "\n",
    "def upload_ngrams(countries):\n",
    "    \n",
    "    for country in countries:\n",
    "        \n",
    "        # vraag de dates op die geprocessed moeten worden\n",
    "        dates = get_remaining_dates(country, get_files_per_date(country))\n",
    "        if len(dates) == 0:\n",
    "            continue\n",
    "            \n",
    "        # dingen voor overzicht in prints\n",
    "        timer = time.time()\n",
    "        total_dates = len(dates.keys())\n",
    "        \n",
    "        def multithread_dates(date):        \n",
    "            paths = get_date_paths(country, dates[date])\n",
    "            \n",
    "            vocabulary = get_date_vocabulary(paths)\n",
    "            counts = get_date_counts(paths, vocabulary)\n",
    "            \n",
    "            elastic_dict = get_es_date_dict(vocabulary, country, date, counts)\n",
    "            \n",
    "            # upload dict to elastic\n",
    "            helpers.bulk(es, elastic_dict)\n",
    "            log_date_upload(country, date)\n",
    "            print(f\"[Info]: {round((time.time() - timer) / 60, 2)}m, Uploaded: {date} {country}\")\n",
    "            \n",
    "        # process multiple dates at the same time\n",
    "        Parallel(n_jobs=15, prefer=\"threads\")(delayed(multithread_dates)(date) for date in dates)\n",
    "            \n",
    "        print(f\"[Info]: {country} finished\")\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7457425",
   "metadata": {},
   "source": [
    "### Progress\n",
    "- If everything is working, you should see print statements of which data and country got Ngrams uploaded to Elasticsearch\n",
    "- This will take about 3 days with `n_jobs=15`, which means 15 dates are being processed at the same time\n",
    "- Change `n_jobs=15` in the cell above if you want more or less processing power dedicated to the Ngram generation task\n",
    "- When everything is done, the cell below will print: `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5fc0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# werkende landen\n",
    "todo = ['AT', 'BA', 'BE', 'BG', 'CZ', 'DK', 'EE', 'ES', 'ES-CT', 'ES-GA', 'ES-PV', 'FI', 'FR', 'GR', 'HR', 'HU', 'IS', 'IT', 'LV', 'NL', 'NO', 'PL', 'PT', 'RS', 'SE', 'SI', 'TR', 'UA', 'GB']\n",
    "\n",
    "done = []\n",
    "\n",
    "upload = upload_ngrams(todo)\n",
    "\n",
    "print(upload)\n",
    "\n",
    "# 4245 minutes in my case --> 'AT', 'BA', 'BE', 'BG', 'CZ', 'DK', 'EE', 'ES', 'ES-CT', 'ES-GA', 'ES-PV', 'FI', 'FR', 'GR', 'HR', 'HU', 'IS', 'IT', 'LV', 'NL', 'NO', 'PL', 'PT', 'RS', 'SE', 'SI', 'TR', 'UA', 'GB'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
